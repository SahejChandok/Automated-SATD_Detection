{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, T5EncoderModel, T5Tokenizer, T5ForConditionalGeneration, DebertaTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "import copy\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.t5.modeling_t5 import T5Config, T5PreTrainedModel, T5Stack\n",
    "from transformers.utils.model_parallel_utils import assert_device_map, get_device_map\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Union, Callable\n",
    "from torch import Tensor\n",
    "from itertools import count \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    make_scorer,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads best model from our training results\n",
    "model = AutoModelForSequenceClassification.from_pretrained('best_model/checkpoint-16000')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"debertav3-base_tokenizer\", model_max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># assume curl will handle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># Is this cacheable?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td># What impact should any(c(\"public\", \"private\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td># Requires validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td># TODO might need to put some params before an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0                          # assume curl will handle\n",
       "1                               # Is this cacheable?\n",
       "2  # What impact should any(c(\"public\", \"private\"...\n",
       "3                              # Requires validation\n",
       "4  # TODO might need to put some params before an..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"satd-comments-manual-subclass.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['comment'].copy()\n",
    "\n",
    "# Remove unhelpful punctuation\n",
    "X = X.apply(lambda x: re.sub('[()#{}<>//=.*:-]', ' ', x).replace('[', ' ').replace(']', ' '))\n",
    "X = X.apply(lambda x: re.sub('\\n', '', x).replace('[', ' ').replace(']', ' '))\n",
    "# Fix double spaces\n",
    "X = X.apply(lambda x: x.replace('  ', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              assume curl will handle\n",
       "1                                   Is this cacheable?\n",
       "2     What impact should any c \"public\", \"private\" ...\n",
       "3                                  Requires validation\n",
       "4     TODO might need to put some params before and...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1] * len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(X.values.tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "class MakeTorchData(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# convert our tokenized data into a torch Dataset\n",
    "test_dataset = MakeTorchData(test_encodings, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2a62f56c210>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU and set seed to ensure reproducability of results. Edit: T5-11b is too big to fit in my GPU :(\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "\n",
    "  predictions, labels = eval_pred\n",
    "  \n",
    "  predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "  score_f1 = f1_score(labels, predictions, pos_label=1, average=\"binary\")\n",
    "  score_acc = accuracy_score(labels, predictions)\n",
    "  score_pre = precision_score(labels, predictions, pos_label=1 , average=\"binary\", sample_weight=None)\n",
    "  score_rec = recall_score(labels, predictions, pos_label=1 , average=\"binary\", sample_weight=None)\n",
    "\n",
    "  return {\"accuracy\": float(score_acc), \"precision\": float(score_pre), \"recall\": float(score_rec), \"f1\": float(score_f1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results2',          # output directory\n",
    "    num_train_epochs=10,     # total number of training epochs\n",
    "    per_device_train_batch_size=4,   # batch size per device during training\n",
    "    per_device_eval_batch_size=4,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)    # select the base metrics\n",
    "    logging_steps=1000,               # log & save weights each logging_steps\n",
    "    save_steps=1000,\n",
    "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=test_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "    #optimizers= (torch.optim.AdamW(model.parameters(), lr=1e-3), None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 4961\n",
      "  Batch size = 4\n",
      "c:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / torch.tensor(\n",
      "c:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:829: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += c2p_att / torch.tensor(scale, dtype=c2p_att.dtype)\n",
      "c:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:852: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010520458221435547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1241,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3b2f01ab29473e8caf5b5ea794b3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = list([np.argmax(x) for x in y_pred[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.000000  0.000000  0.000000         0\n",
      "           1   1.000000  0.509978  0.675477      4961\n",
      "\n",
      "    accuracy                       0.509978      4961\n",
      "   macro avg   0.500000  0.254989  0.337739      4961\n",
      "weighted avg   1.000000  0.509978  0.675477      4961\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y, y_pred2, digits=6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4593b52a31704b5155580280f0f05a500adb36bf97584c7e94e26dfe17ea16d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
